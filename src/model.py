from .message import Message
from .tool import Tool, ToolCall
from openai import OpenAI
from ollama import chat
from dotenv import load_dotenv

import json

load_dotenv()

class Model:
    def generate(self, messages: list[Message], tools = None) -> Message:
        pass

class OpenAIModel:
    """OAI env vars need to be set in .env"""
    def __init__(self, model_type="gpt-3.5-turbo"):
        self._model = OpenAI()
        self.model_type = model_type

    def generate(self, input: str):
        return self._generate(input)

    def tool_to_openai_tool(self, tool: Tool):
        return {"type": "function"}

    def _make_tool_output_message(self, oai_tool_call, output):
        return {
            "type": "function_call_output",
            "call_id": oai_tool_call["call_id"],
            "output": str(output)
        }

    def _messages_to_dict(self, messages: list[Message]):
        """Converts our Message object into format expected by model"""
        # TODO: Rename fn to reflect functionality
        oai_messages = []
        for m in messages:
            if m.type == "text":
                oai_messages.append({"role": m.role, "content": m.content})
            elif m.type == "tool_call":
                raise ValueError(
                    "Tool call messages are only generated by the model, "
                    "only tool call outputs should be passed to it"
                )
            else: # type is tool_output
                # first add tool call messages
                oai_tool_calls = m.tool_call_msg._orig_msg
                oai_messages += oai_tool_calls
                for tool_output, oai_tool_call in zip(m.tool_outputs.values(), oai_tool_calls):
                    oai_messages.append(
                        self._make_tool_output_message(oai_tool_call, tool_output)
                    )

        return oai_messages

    def _llm_output_to_message(self, llm_response):
        output = llm_response.output

        message=None
        # making sure things are as I think they are, will clean up once I confirm
        if any(o["type"] == "function_call" for o in output): # then all the outputs should be tool calls, create tool_call message
            tool_calls = []
            for o in output:
                fn_name = o["name"]
                params = json.loads(o["arguments"])
                tool_calls.append(ToolCall(fn_name, params))
            message = Message("assistant", None, "tool_call", tool_calls, output)
            if any(o["type"] != "function_call" for o in output):
                print("Mix of tool calls and text messages, Mackenzie needs to fix")
        elif len(output) > 1:
            print("Error: No tool calls in output (so all output is text) but there is more than message in output, Mackenzie needs to fix")
        else: # message is single text output as expected if there are no function calls
            message = Message("assistant", output[0]["content"]["text"])
        
        return message

    def _generate(self, messages, tools = None):
        # TODO: retry on diff errors (rate limit, connection failure)

        if tools is not None:
            tools = [self.tool_to_openai_tool(t) for t in tools]

        system_prompt = None
        if messages[0].role == "system":
            system_prompt = messages[0].content
            messages = messages[1:]

        model_output = self._model.responses.create(
            instructions=system_prompt,
            messages=self._messages_to_dict(messages),
            model=self.model_type,
            tools=tools
        )

        return self._llm_output_to_message(model_output)

# TODO: finish implementing
class LLamaModel:
    def __init__(self, model_type):
        self.model_type = model_type
    
    def tool_to_llama_tool(self, tool: Tool):
        return {"type": "function"}

    def _messages_to_dict(self, messages) -> dict:
        pass

    def generate(self, messages, tools) -> Message:
        if tools is not None:
            tools = [self.tool_to_llama_tool(t) for t in tools]

        chat_completion = chat(
            self.model_type,
            self._messages_to_dict(messages),
            tools=tools
        )

        return Message("assistant", chat_completion.message.content)

    
if __name__ == "__main__":
    assert(OpenAIModel().generate("Say only the word 'Hello'") == 'Hello')