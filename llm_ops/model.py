from llm_ops.message import Message, ToolCallMessage
from llm_ops.tool import Tool, ToolCall
from openai import OpenAI
from ollama import chat
from dotenv import load_dotenv

import json

load_dotenv()

class Model:
    def generate(self, messages: list[Message], tools = None) -> Message:
        pass

class OpenAIModel:
    """OAI env vars need to be set in .env"""
    def __init__(self, model_type="gpt-3.5-turbo"):
        self._model = OpenAI()
        self.model_type = model_type

    def tool_to_openai_tool(self, tool: Tool): 
        args_schema = tool.args_schema
        return {
            "type": "function",
            "name": tool.name,
            "description": tool.description,
            "parameters": {
                "type": "object",
                "properties": args_schema,
                "required": [k for k in args_schema if "default" not in args_schema[k]],
                "additionalProperties": False
            }
        }

    def _make_tool_output_message(self, oai_tool_call, output):
        return {
            "type": "function_call_output",
            "call_id": oai_tool_call.call_id,
            "output": str(output)
        }

    def _messages_to_dict(self, messages: list[Message]):
        """Converts our Message object into format expected by model"""
        # TODO: Rename fn to reflect functionality
        oai_messages = []
        for m in messages:
            if m.type == "text":
                oai_messages.append({"role": m.role, "content": m.content})
            elif m.type == "tool_call":
                raise ValueError(
                    "Tool call messages are only generated by the model, "
                    "only tool call outputs should be passed to it"
                )
            else: # type is tool_output
                # first add tool call messages
                oai_tool_calls = m.tool_call_msg._orig_msg
                oai_messages += oai_tool_calls
                for tool_output, oai_tool_call in zip(m.tool_outputs, oai_tool_calls):
                    oai_messages.append(
                        self._make_tool_output_message(oai_tool_call, tool_output)
                    )

        return oai_messages

    def _llm_output_to_message(self, llm_response):
        output = llm_response.output

        message=None
        # making sure things are as I think they are, will clean up once I confirm
        if any(o.type == "function_call" for o in output): # then all the outputs should be tool calls, create tool_call message
            tool_calls = []
            for o in output:
                fn_name = o.name
                params = json.loads(o.arguments)
                tool_calls.append(ToolCall(fn_name, params))
            message = ToolCallMessage(tool_calls, output)
            if any(o.type != "function_call" for o in output):
                print("Mix of tool calls and text messages, Mackenzie needs to fix")
        elif len(output) > 1:
            print("Error: No tool calls in output (so all output is text) but there is more than message in output, Mackenzie needs to fix")
        else: # message is text output as expected if there are no function calls
            message = Message("assistant", llm_response.output_text)
        
        return message

    def generate(self, messages, tools = None):
        # TODO: retry on diff errors (rate limit, connection failure)

        if tools is not None:
            tools = [self.tool_to_openai_tool(t) for t in tools]

        system_prompt = None
        if messages[0].role == "system":
            system_prompt = messages[0].content
            messages = messages[1:]

        model_output = self._model.responses.create(
            instructions=system_prompt,
            input=self._messages_to_dict(messages),
            model=self.model_type,
            tools=tools
        )
        

        return self._llm_output_to_message(model_output)

# TODO: finish implementing
class LLamaModel:
    def __init__(self, model_type):
        self.model_type = model_type
    
    def tool_to_llama_tool(self, tool: Tool):
        return {"type": "function"}

    def _messages_to_dict(self, messages) -> dict:
        pass

    def generate(self, messages, tools) -> Message:
        if tools is not None:
            tools = [self.tool_to_llama_tool(t) for t in tools]

        chat_completion = chat(
            self.model_type,
            self._messages_to_dict(messages),
            tools=tools
        )

        return Message("assistant", chat_completion.message.content)

    
if __name__ == "__main__":
    import openai
    print(openai.__version__)
    msgs = [Message("user", "Say only the word 'Hello'")]
    assert(OpenAIModel().generate(msgs).content == 'Hello')


    @Tool.from_fn
    def test_fn(a: int) -> int:
        """This is a function for testing function calling.
        
        Args:
            a (int): This is the sole argument to the function
        
        Returns:
            int: The thing returned by the function
        """
        return a
    
    print(test_fn.arg_types)
    
    msgs = [Message("user", "Please call the function test_fn with a=3 and print the result")]
    response = OpenAIModel().generate(msgs, tools=[test_fn])
    assert(type(response) == ToolCallMessage)
    response_out = response.to_tool_output([test_fn])
    print(OpenAIModel().generate(msgs + [response_out], tools=[test_fn]).content)
    exit()
    assert(OpenAIModel().generate(msgs + [response_out], tools=[test_fn]).content == "3")

